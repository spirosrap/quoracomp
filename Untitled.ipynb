{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torchtext\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "tqdm.pandas(desc='Progress')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")\n",
    "df = pd.concat([df_train ,df_test],sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embed(file):\n",
    "    def get_coefs(word,*arr): \n",
    "        return word, np.asarray(arr, dtype='float32')\n",
    "    \n",
    "    if file == '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec':\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file) if len(o)>100)\n",
    "    else:\n",
    "        embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(file, encoding='latin'))\n",
    "        \n",
    "    return embeddings_index\n",
    "\n",
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words\n",
    "\n",
    "def known_contractions(embed):\n",
    "    known = []\n",
    "    for contract in contraction_mapping:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x\n",
    "def unknown_punct(embed, punct):\n",
    "    unknown = ''\n",
    "    for p in punct:\n",
    "        if p not in embed:\n",
    "            unknown += p\n",
    "            unknown += ' '\n",
    "    return unknown\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "paragram =  '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "wiki_news = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting GloVe embedding\n"
     ]
    }
   ],
   "source": [
    "print(\"Extracting GloVe embedding\")\n",
    "embed_glove = load_embed(glove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(df['question_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1362492/1362492 [00:01<00:00, 1117235.77it/s]\n"
     ]
    }
   ],
   "source": [
    "df['lowered_question'] = df['question_text'].progress_apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1362492/1362492 [00:04<00:00, 326724.34it/s]\n"
     ]
    }
   ],
   "source": [
    "df['treated_question'] = df['lowered_question'].progress_apply(lambda x: clean_contractions(x, contraction_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 15199 words to embedding\n"
     ]
    }
   ],
   "source": [
    "add_lower(embed_glove, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1362492/1362492 [00:17<00:00, 78345.62it/s]\n"
     ]
    }
   ],
   "source": [
    "df['treated_question'] = df['treated_question'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1362492/1362492 [00:09<00:00, 144564.25it/s]\n"
     ]
    }
   ],
   "source": [
    "df['treated_question'] = df['treated_question'].progress_apply(lambda x: correct_spelling(x, mispell_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = build_vocab(df['treated_question'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['question_text'] = df_train['question_text'].progress_apply(lambda x: x.lower())\n",
    "df_train['question_text'] = df_train['question_text'].progress_apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "df_train['question_text'] =df_train['question_text'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n",
    "df_train['question_text'] = df_train['question_text'].progress_apply(lambda x: correct_spelling(x, mispell_dict))\n",
    "\n",
    "df_test['question_text'] = df_test['question_text'].progress_apply(lambda x: x.lower())\n",
    "df_test['question_text'] = df_test['question_text'].progress_apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "df_test['question_text'] =df_test['question_text'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n",
    "df_test['question_text'] = df_test['question_text'].progress_apply(lambda x: correct_spelling(x, mispell_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.0648e-01, -1.6295e-02, -2.2755e-01, -1.8934e-01,  1.4167e-01,\n",
       "        2.7404e-01, -2.8969e-02, -3.1540e-01, -4.0320e-01,  2.7085e+00,\n",
       "        1.5321e-01,  1.5345e-01, -8.5798e-02, -1.7395e-01, -5.9932e-03,\n",
       "       -4.4821e-02, -2.7702e-02,  1.0713e+00, -3.1542e-01, -2.5109e-01,\n",
       "       -3.1534e-01,  1.4133e-02, -3.8718e-01,  1.3518e-01, -6.7774e-02,\n",
       "        3.4392e-01, -2.0210e-01,  1.7084e-03,  5.3716e-02, -6.6964e-02,\n",
       "        1.2081e-01,  2.1121e-01,  3.6278e-02,  1.0395e-01,  1.3312e-01,\n",
       "       -2.9282e-01,  1.2094e-01,  9.6785e-02, -1.3612e-01, -1.4153e-01,\n",
       "       -3.5055e-03,  2.3564e-01,  4.4989e-02, -2.9994e-03, -6.0647e-02,\n",
       "       -1.5818e-01, -3.0980e-01, -2.5685e-01, -7.7777e-02,  1.0374e-01,\n",
       "       -3.2998e-02,  1.6710e-01, -1.7314e-01,  1.1451e-01,  6.6825e-02,\n",
       "       -7.6626e-04,  2.1958e-02, -2.8378e-02, -1.4265e-01, -2.4112e-01,\n",
       "        1.1367e-01, -1.0256e-01,  2.3031e-02, -2.7696e-02, -6.9263e-02,\n",
       "       -1.5693e-01, -2.6229e-02, -1.4117e-01, -5.0208e-02,  7.2067e-02,\n",
       "        1.3692e-01,  1.6498e-01, -8.9294e-02,  8.2570e-02, -1.2619e-01,\n",
       "        2.9701e-01,  1.7990e-01, -1.2885e-01,  1.5327e-01,  2.0220e-01,\n",
       "       -4.0822e-02,  1.2186e-01, -2.6206e-01, -1.8238e-03,  3.2863e-01,\n",
       "       -3.7791e-01, -1.7301e-01, -8.5302e-02, -1.2740e-01, -1.2985e-01,\n",
       "        9.9124e-02,  1.2756e-01,  2.9430e-01,  1.7452e-01,  2.6084e-01,\n",
       "       -4.0348e-01,  1.8599e-01, -2.5534e-01,  3.5389e-02, -3.7783e-01,\n",
       "       -1.1814e-01,  7.1124e-02, -2.9684e-01, -2.1380e-01, -1.3155e-01,\n",
       "       -8.6324e-01, -1.0318e-01, -3.8374e-02,  1.3517e-01, -4.1433e-02,\n",
       "       -1.6216e-01, -1.1313e-01,  8.7611e-02, -1.1195e-01,  6.4373e-02,\n",
       "       -2.1678e-01, -6.1492e-02,  6.7132e-02, -2.9096e-02,  6.8107e-03,\n",
       "       -4.5581e-02, -1.5130e-02, -2.0084e-01, -1.2069e-02, -9.3236e-02,\n",
       "        2.0572e-01,  4.4586e-02, -2.0840e-02, -9.6290e-02,  7.8893e-02,\n",
       "        2.3459e-02, -6.8620e-02,  5.9130e-02,  2.1674e-01, -7.8315e-02,\n",
       "        2.1363e-01,  1.4314e-01, -1.1269e-01,  2.1795e-01,  1.3473e-01,\n",
       "       -9.6633e-01,  8.9384e-02,  5.4565e-01, -1.1909e-01,  2.6468e-02,\n",
       "       -1.5632e-02,  4.3335e-01, -6.3477e-02,  1.7602e-01, -1.1200e-01,\n",
       "       -5.0678e-02,  9.7400e-02,  1.0835e-01, -2.2863e-02, -7.9371e-02,\n",
       "       -1.8613e-01, -9.5579e-02,  9.6452e-02,  1.6985e-01, -3.1720e-01,\n",
       "       -1.0339e-01, -1.3775e-01,  1.3053e-01, -3.7955e-01, -9.1040e-02,\n",
       "       -4.7089e-01, -2.1285e-02,  1.3461e-01,  1.8898e-01, -1.3175e-01,\n",
       "        7.2037e-02,  1.0228e-01,  5.2890e-02,  1.0862e-01,  8.3393e-03,\n",
       "        6.4497e-03,  6.2593e-02,  1.0614e-02, -4.9213e-01, -8.1617e-02,\n",
       "       -1.6147e-02,  1.9622e-02, -9.7198e-02, -2.8358e-01, -3.0155e-01,\n",
       "        1.1693e-02, -1.5195e-01,  1.6121e-01,  2.5092e-01, -1.4530e-02,\n",
       "       -7.1483e-02,  6.8883e-02, -2.1714e-02,  1.1604e-01, -1.5646e-01,\n",
       "       -6.8977e-02, -2.1630e-01,  1.9400e-02, -8.7162e-02,  2.3781e-01,\n",
       "        9.2504e-02, -4.8853e-02, -1.8378e-01,  1.4822e-01,  2.0719e-01,\n",
       "        1.4652e-01,  1.8217e-01, -1.2638e-01, -2.0504e-01,  2.0200e-01,\n",
       "       -1.6742e-01, -7.4659e-02,  7.0591e-02, -2.8141e-01, -9.6293e-02,\n",
       "        3.0172e-01,  1.9460e-01,  3.6951e-01, -1.7860e-01, -2.3215e-01,\n",
       "        4.3395e-02,  4.2423e-01,  4.2782e-03,  1.5525e-01,  4.5669e-02,\n",
       "        2.9934e-01, -3.5877e-02,  2.7884e-01, -7.0911e-02,  1.4240e-01,\n",
       "       -3.3601e-01, -3.6872e-02,  7.7823e-02,  1.7128e-03, -1.4294e-01,\n",
       "       -3.5338e-01,  8.6800e-02, -1.4035e-01, -1.2714e-02,  7.5596e-02,\n",
       "        2.1089e-01,  1.8058e-01, -4.3142e-02, -9.8158e-03,  4.4935e-01,\n",
       "       -1.2383e-01, -1.6641e-01, -2.2360e-01,  1.5068e-01, -1.1659e-01,\n",
       "        2.2787e-01,  1.5582e-01,  1.7690e-02, -1.9982e-02,  8.6623e-02,\n",
       "        1.8547e-01,  2.8375e-01,  5.9658e-02,  9.3334e-02, -7.0569e-02,\n",
       "        3.9621e-01,  4.1797e-01, -1.1218e-02,  3.1010e-01,  2.0182e-02,\n",
       "        6.7417e-03, -3.5264e-02, -3.9675e-01,  5.8749e-01, -1.6479e-01,\n",
       "        3.0791e-01, -3.0143e-01, -2.6626e-01,  2.2925e-02, -1.6963e-01,\n",
       "        1.2999e-01,  2.1843e-03,  1.9784e-01, -1.0350e-01,  3.3119e-01,\n",
       "        2.8959e-01,  2.0309e-01,  2.2411e-01, -1.6620e-01, -4.4593e-02,\n",
       "       -2.6372e-01,  9.4942e-02, -3.2548e-01,  4.3998e-01,  1.3719e-01,\n",
       "       -1.5922e-01,  1.8396e-01,  5.5808e-02, -1.2251e-01, -1.3090e-01,\n",
       "        1.6070e-01, -6.1876e-02, -3.1343e-01,  8.7424e-02, -1.6610e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_glove[\"as\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
