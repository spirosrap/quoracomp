{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## IMPORTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from torchtext import data\n",
    "import spacy\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "tqdm.pandas(desc='Progress')\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from torchtext.data import Example\n",
    "from sklearn.metrics import f1_score\n",
    "import torchtext\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# load spacy tokenizer\n",
    "nlp = spacy.load('en',disable=['parser', 'tagger', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOAD PROCESSED TRAINING DATA FROM DISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# insincere questions: 1,225,312(93.81%) and # sincere questions: 80,810(6.19%)\n",
      "# Test samples: 56,370(0.043% of train samples)\n"
     ]
    }
   ],
   "source": [
    "sin = len(df_train[df_train[\"target\"]==0])\n",
    "insin = len(df_train[df_train[\"target\"]==1])\n",
    "persin = (sin/(sin+insin))*100\n",
    "perinsin = (insin/(sin+insin))*100            \n",
    "print(\"# insincere questions: {:,}({:.2f}%) and # sincere questions: {:,}({:.2f}%)\".format(sin,persin,insin,perinsin))\n",
    "# print(\"Sinsere:{}% Insincere: {}%\".format(round(persin,2),round(perinsin,2)))\n",
    "print(\"# Test samples: {:,}({:.3f}% of train samples)\".format(len(df_test),len(df_test)/len(df_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"question_text\"] = df_train[\"question_text\"].fillna(\"_na_\").values\n",
    "df_test[\"question_text\"] = df_test[\"question_text\"].fillna(\"_na_\").values\n",
    "\n",
    "df_train.to_csv(\"train2.csv\")\n",
    "df_test.to_csv(\"test2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>qid</th>\n",
       "      <th>question_text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00002165364db923c7e6</td>\n",
       "      <td>How did Quebec nationalists see their province...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000032939017120e6e44</td>\n",
       "      <td>Do you have an adopted dog, how would you enco...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0000412ca6e4628ce2cf</td>\n",
       "      <td>Why does velocity affect time? Does velocity a...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000042bf85aa498cd78e</td>\n",
       "      <td>How did Otto von Guericke used the Magdeburg h...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0000455dfa3e01eae3af</td>\n",
       "      <td>Can I convert montra helicon D to a mountain b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    qid                                      question_text  \\\n",
       "0  00002165364db923c7e6  How did Quebec nationalists see their province...   \n",
       "1  000032939017120e6e44  Do you have an adopted dog, how would you enco...   \n",
       "2  0000412ca6e4628ce2cf  Why does velocity affect time? Does velocity a...   \n",
       "3  000042bf85aa498cd78e  How did Otto von Guericke used the Magdeburg h...   \n",
       "4  0000455dfa3e01eae3af  Can I convert montra helicon D to a mountain b...   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 342,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a pytorch dataset from the train samples and build a vocabulary using embedding vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load dataframe to csv\n",
    "\n",
    "TEXT = data.Field(lower=True, batch_first=True,tokenize='spacy')#preprocessing=generate_bigrams)\n",
    "LABEL = data.LabelField(dtype=torch.float)\n",
    "qid = data.Field()\n",
    "\n",
    "train_dataset = data.TabularDataset(path='train2.csv', format='csv',\n",
    "                                      fields={'question_text': ('text',TEXT),\n",
    "                                              'target': ('label',LABEL)})\n",
    "final_test_dataset = data.TabularDataset(path='test2.csv', format='csv',\n",
    "                                     fields={'qid': ('qid', qid),\n",
    "                                             'question_text': ('text', TEXT)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT.build_vocab(train_dataset, final_test_dataset, min_freq=3)\n",
    "qid.build_vocab(final_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "vec = torchtext.vocab.Vectors('../input/embeddings/glove.840B.300d/glove.840B.300d.txt', cache='./cache/')\n",
    "# vec = torchtext.vocab.Vectors('wiki-news-300d-1M/wiki-news-300d-1M.vec', cache='./cache/')\n",
    "TEXT.vocab.load_vectors(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([72017, 300])"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEXT.build_vocab(train_dataset, max_size=50000, vectors=vec)\n",
    "LABEL.build_vocab(train_dataset)\n",
    "TEXT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SPLIT DATA TO TRAINiNG AND VALIDATION SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_dataset, test_dataset = train_dataset.split(split_ratio=0.9,random_state=random.seed(SEED))\n",
    "train_dataset, valid_dataset = train_dataset.split(split_ratio=0.9,random_state=random.seed(SEED))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 512\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "train_iterator = data.BucketIterator(\n",
    "    train_dataset, \n",
    "    batch_size=BATCH_SIZE,sort_key=lambda x: len(x.text),shuffle=True,sort=False, \n",
    "    device=device)\n",
    "# test_iterator = data.BucketIterator(\n",
    "#     test_dataset, \n",
    "#     batch_size=BATCH_SIZE,sort_key=lambda x: len(x.text),train=False,sort=False,\n",
    "#     device=device)\n",
    "valid_iterator = data.BucketIterator(\n",
    "    valid_dataset, \n",
    "    batch_size=BATCH_SIZE,sort_key=lambda x: len(x.text),train=False,sort=False, \n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Attention(nn.Module):\n",
    "\t'''\n",
    "\tClass to define the attention network component (attention is computed over the entire sequence length so\n",
    "\tthat the information from each time step can be used in the final prediction and not just the final time step)\n",
    "\tThe network is a 3 layer MLP architecture followed by a softmax\n",
    "\tArguments:\n",
    "\t\tseq_len : maximum sequence length (this is required to predefine the dimensions of the network)\n",
    "\t\thidden_emb : same as the hidden dimension of the lstm network\n",
    "\tReturns:\n",
    "\t\tNone\n",
    "\t'''\n",
    "\tdef __init__(self, seq_len=18, hidden_emb=1024):\n",
    "\t\tsuper(Attention, self).__init__()\n",
    "\n",
    "\t\tself.seq_len = seq_len\n",
    "\t\tself.hidden_emb = hidden_emb\n",
    "\t\tself.mlp1_units = 3072\n",
    "\t\tself.mlp2_units = 1024\n",
    "\n",
    "\t\tself.fc = nn.Sequential(\n",
    "            nn.Linear(self.seq_len*self.hidden_emb, self.mlp1_units),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.mlp1_units, self.mlp2_units),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=0.5),\n",
    "            nn.Linear(self.mlp2_units, self.seq_len),\n",
    "            nn.ReLU(inplace=True),\n",
    "            )\n",
    "\n",
    "\t\tself.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "\t'''\n",
    "\tComputes the attention on the lstm outputs from each time step in the sequence\n",
    "\tArguments:\n",
    "\t\tlstm_emd : lstm embedding from each time step in the sequence\n",
    "\tReturns:\n",
    "\t\tattn_feature_map : embedding computed after applying attention to the lstm embedding of the entire sequence\n",
    "\t'''\n",
    "\tdef forward(self, lstm_emd):\n",
    "\n",
    "\t\tbatch_size = lstm_emd.shape[0]\n",
    "\t\tlstm_emd = lstm_emd.contiguous()\n",
    "\t\tlstm_flattened = lstm_emd.view(batch_size, -1) # to pass it to the MLP architecture\n",
    "\n",
    "\t\tattn = self.fc(lstm_flattened) # attention over the sequence length\n",
    "\t\talpha = self.softmax(attn) # gives the probability values for the time steps in the sequence (weights to each time step)\n",
    "\n",
    "\t\talpha = torch.stack([alpha]*self.mlp2_units, dim=2) # stack it across the lstm embedding dimesion\n",
    "\n",
    "\t\tattn_feature_map = lstm_emd * alpha # gives attention weighted lstm embedding\n",
    "\t\tattn_feature_map = torch.sum(attn_feature_map, dim=1, keepdim=True) # computes the weighted sum\n",
    "\t\treturn attn_feature_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttnDecoderRNN(nn.Module):\n",
    "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=100):\n",
    "        super(AttnDecoderRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "        self.dropout_p = dropout_p\n",
    "        self.max_length = max_length\n",
    "\n",
    "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
    "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
    "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
    "        self.dropout = nn.Dropout(self.dropout_p)\n",
    "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
    "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
    "\n",
    "    def forward(self, input, hidden, encoder_outputs):\n",
    "        embedded = self.embedding(input).view(1, 1, -1)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        attn_weights = F.softmax(\n",
    "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
    "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
    "                                 encoder_outputs.unsqueeze(0))\n",
    "\n",
    "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
    "        output = self.attn_combine(output).unsqueeze(0)\n",
    "\n",
    "        output = F.relu(output)\n",
    "        output, hidden = self.gru(output, hidden)\n",
    "\n",
    "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
    "        return output, hidden, attn_weights\n",
    "\n",
    "    def initHidden(self):\n",
    "        return torch.zeros(1, 1, self.hidden_size, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, pretrained_lm, padding_idx, static=True, hidden_dim=128, lstm_layer=2, dropout=0.2):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embedding = nn.Embedding.from_pretrained(pretrained_lm)\n",
    "        self.embedding.padding_idx = padding_idx\n",
    "        if static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=lstm_layer, \n",
    "                            dropout = dropout,\n",
    "                            bidirectional=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim*lstm_layer*2, 1)\n",
    "        self.fc = nn.Linear(128*2, 1)\n",
    "        self.attention = SelfAttention(128*2, batch_first=True)\n",
    "\n",
    "    def forward(self, sents):\n",
    "        x = self.embedding(sents)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.transpose(x, dim0=1, dim1=0)\n",
    "#         self.out = nn.Linear(hidden_dim, num_classes)\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        y = self.hidden2label(self.dropout(torch.cat([c_n[i,:, :] for i in range(c_n.shape[0])], dim=1)))\n",
    "        return y    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (dropout): Dropout(p=0.2)\n",
      "  (embedding): Embedding(72017, 300, padding_idx=1)\n",
      "  (lstm): LSTM(300, 128, num_layers=2, dropout=0.2, bidirectional=True)\n",
      "  (hidden2label): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
      "  (attention): SelfAttention()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.5\n",
    "\n",
    "# model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
    "model = BiLSTM(TEXT.vocab.vectors, lstm_layer=2, padding_idx=TEXT.vocab.stoi[TEXT.pad_token], hidden_dim=128,dropout=0.2).cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "        [-0.0869,  0.1916,  0.1091,  ..., -0.0152,  0.1111,  0.2065],\n",
       "        ...,\n",
       "        [-0.2523, -0.1560, -0.0008,  ...,  0.0601,  0.3452,  0.2371],\n",
       "        [ 0.0202,  0.1975, -0.0793,  ...,  0.0901, -0.6364,  0.2416],\n",
       "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 362,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pretrained_embeddings = TEXT.vocab.vectors\n",
    "\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters())\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()),\n",
    "                    lr=1e-3)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "def binary_accuracy(preds, y, th = 0.5):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    acc = correct.sum()/len(correct)\n",
    "    return acc\n",
    "\n",
    "def f1_score_model(preds, y,th = 0.5):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8\n",
    "    \"\"\"\n",
    "    rounded = torch.sigmoid(preds).cpu().apply_(lambda x: 1 if x>=th else 0)\n",
    "    rounded_preds = rounded\n",
    "\n",
    "    return f1_score(y.cpu().numpy(),rounded_preds.cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_training = 0\n",
    "warmup_epoch = 2\n",
    "step = 0\n",
    "max_loss = 1e5\n",
    "no_improve_in_previous_epoch = False\n",
    "no_improve_epoch = 0\n",
    "fine_tuning = False\n",
    "train_record = []\n",
    "val_record = []\n",
    "losses = []\n",
    "\n",
    "def train(model, iterator, optimizer, criterion,e):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    global warm_epoch\n",
    "    global no_improve_in_previous_epoch\n",
    "    global fine_tuning\n",
    "    global step\n",
    "    global max_loss\n",
    "    global stop_training\n",
    "    global no_improve_epoch\n",
    "    global train_record\n",
    "    global val_record\n",
    "    global losses\n",
    "    \n",
    "    model.train()\n",
    "    if e >= warmup_epoch:\n",
    "        if no_improve_in_previous_epoch:\n",
    "            no_improve_epoch += 1\n",
    "            if no_improve_epoch >= 1:\n",
    "                stop_training = 1\n",
    "        else:\n",
    "            no_improve_epoch = 0\n",
    "        no_improve_in_previous_epoch = True\n",
    "    if stop_training == 0:    \n",
    "        if not fine_tuning and e >= warmup_epoch:\n",
    "            model.embedding.weight.requires_grad = True        \n",
    "            fine_tuning = True\n",
    "\n",
    "        for batch in iterator:\n",
    "            step += 1\n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            predictions = model(batch.text).squeeze(1)       \n",
    "\n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "            if step % 500 == 0:\n",
    "                model.eval()\n",
    "                model.zero_grad()\n",
    "                val_loss = []\n",
    "                for val_batch in iter(valid_iterator):\n",
    "                    val_x = val_batch.text.cuda()\n",
    "                    val_y = val_batch.label.type(torch.Tensor).cuda()\n",
    "                    val_pred = model.forward(val_x).view(-1)\n",
    "                    val_loss.append(criterion(val_pred, val_y).cpu().data.numpy())\n",
    "                val_record.append({'step': step, 'loss': np.mean(val_loss)})\n",
    "                print('epoch {:02} - step {:06} - train_loss {:.4f} - val_loss {:.4f} '.format(\n",
    "                            e+1, step, np.mean(losses), val_record[-1]['loss']))\n",
    "                if e >= warmup_epoch:\n",
    "                    if val_record[-1]['loss'] <= max_loss:\n",
    "                        save(m=model, info={'step': step, 'epoch': e+1, 'train_loss': np.mean(losses),\n",
    "                                            'val_loss': val_record[-1]['loss']})\n",
    "                        max_loss = val_record[-1]['loss']\n",
    "                        no_improve_in_previous_epoch = False\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            loss = criterion(predictions, batch.label)\n",
    "\n",
    "            acc = binary_accuracy(predictions, batch.label)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate_f1(model, iterator, criterion,th=0.5):\n",
    "    \n",
    "    f1_scores = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for batch in iterator:\n",
    "\n",
    "            predictions = model(batch.text).squeeze(1)\n",
    "            \n",
    "            f1 = f1_score_model(predictions, batch.label,th=th)\n",
    "            f1_scores.append(f1)\n",
    "        \n",
    "    return np.array(f1_scores).mean()\n",
    "\n",
    "\n",
    "def save(m, info):\n",
    "    torch.save(info, 'best_model.info')\n",
    "    torch.save(m, 'best_model.m')\n",
    "    \n",
    "def load():\n",
    "    m = torch.load('best_model.m')\n",
    "    info = torch.load('best_model.info')\n",
    "    return m, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 01 - step 000500 - train_loss nan - val_loss 0.1190 \n",
      "epoch 01 - step 001000 - train_loss nan - val_loss 0.1153 \n",
      "epoch 01 - step 001500 - train_loss nan - val_loss 0.1090 \n",
      "epoch 01 - step 002000 - train_loss nan - val_loss 0.1067 \n",
      "| Epoch: 01 | Train Loss: 0.120 | Train Acc: 95.34% | Val. Loss: 0.107 | Val. Acc: 95.78% |\n",
      "epoch 02 - step 002500 - train_loss nan - val_loss 0.1050 \n",
      "epoch 02 - step 003000 - train_loss nan - val_loss 0.1052 \n",
      "epoch 02 - step 003500 - train_loss nan - val_loss 0.1035 \n",
      "epoch 02 - step 004000 - train_loss nan - val_loss 0.1027 \n",
      "| Epoch: 02 | Train Loss: 0.104 | Train Acc: 95.87% | Val. Loss: 0.102 | Val. Acc: 95.94% |\n",
      "epoch 03 - step 004500 - train_loss nan - val_loss 0.1023 \n",
      "epoch 03 - step 005000 - train_loss nan - val_loss 0.1007 \n",
      "epoch 03 - step 005500 - train_loss nan - val_loss 0.1013 \n",
      "epoch 03 - step 006000 - train_loss nan - val_loss 0.1003 \n",
      "| Epoch: 03 | Train Loss: 0.099 | Train Acc: 96.06% | Val. Loss: 0.100 | Val. Acc: 96.02% |\n",
      "epoch 04 - step 006500 - train_loss nan - val_loss 0.0999 \n",
      "epoch 04 - step 007000 - train_loss nan - val_loss 0.1011 \n",
      "epoch 04 - step 007500 - train_loss nan - val_loss 0.0990 \n",
      "epoch 04 - step 008000 - train_loss nan - val_loss 0.0987 \n",
      "| Epoch: 04 | Train Loss: 0.094 | Train Acc: 96.22% | Val. Loss: 0.099 | Val. Acc: 96.09% |\n",
      "epoch 05 - step 008500 - train_loss nan - val_loss 0.0989 \n",
      "epoch 05 - step 009000 - train_loss nan - val_loss 0.0990 \n",
      "epoch 05 - step 009500 - train_loss nan - val_loss 0.0998 \n",
      "epoch 05 - step 010000 - train_loss nan - val_loss 0.0978 \n",
      "| Epoch: 05 | Train Loss: 0.090 | Train Acc: 96.35% | Val. Loss: 0.098 | Val. Acc: 96.08% |\n",
      "epoch 06 - step 010500 - train_loss nan - val_loss 0.1009 \n",
      "epoch 06 - step 011000 - train_loss nan - val_loss 0.1014 \n",
      "epoch 06 - step 011500 - train_loss nan - val_loss 0.1004 \n",
      "epoch 06 - step 012000 - train_loss nan - val_loss 0.0979 \n",
      "| Epoch: 06 | Train Loss: 0.086 | Train Acc: 96.51% | Val. Loss: 0.099 | Val. Acc: 96.05% |\n",
      "| Epoch: 07 | Train Loss: 0.000 | Train Acc: 0.00% | Val. Loss: 0.099 | Val. Acc: 96.05% |\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 10\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion,e=epoch)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    print(f'| Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}% | Val. Loss: {valid_loss:.3f} | Val. Acc: {valid_acc*100:.2f}% |')\n",
    "    if stop_training == 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'step': 10000, 'epoch': 5, 'train_loss': nan, 'val_loss': 0.097822756}"
      ]
     },
     "execution_count": 366,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model, m_info = load()\n",
    "m_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 0.098 | Test Acc: 96.13% |\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}% |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| f1 score: 0.645\n"
     ]
    }
   ],
   "source": [
    "score = evaluate_f1(model, valid_iterator, criterion)\n",
    "print(f'| f1 score: {score:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "val_pred = []\n",
    "val_true = []\n",
    "valid_iterator.init_epoch()\n",
    "for val_batch in iter(valid_iterator):\n",
    "    val_x = val_batch.text.cuda()\n",
    "    val_true += val_batch.label.cpu().data.numpy().tolist()\n",
    "    val_pred += torch.sigmoid(model.forward(val_x).view(-1)).cpu().data.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold is 0.3300 with F1 score: 0.6799\n"
     ]
    }
   ],
   "source": [
    "tmp = [0,0,0] # idx, cur, max\n",
    "delta = 0\n",
    "for tmp[0] in np.arange(0.1, 0.501, 0.01):\n",
    "    tmp[1] = f1_score(val_true, np.array(val_pred)>tmp[0])\n",
    "    if tmp[1] > tmp[2]:\n",
    "        delta = tmp[0]\n",
    "        tmp[2] = tmp[1]\n",
    "print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.zero_grad()\n",
    "test_pred = []\n",
    "test_id = []\n",
    "\n",
    "final_test_iterator = torchtext.data.BucketIterator(dataset=final_test_dataset,\n",
    "                                    batch_size=BATCH_SIZE,\n",
    "                                    sort_key=lambda x: x.text.__len__(),train=False,sort=False)\n",
    "\n",
    "\n",
    "for test_batch in iter(final_test_iterator):\n",
    "    test_x = test_batch.text.cuda()\n",
    "    test_pred += torch.sigmoid(model.forward(test_x).view(-1)).cpu().data.numpy().tolist()\n",
    "    test_id += test_batch.qid.view(-1).data.numpy().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df =pd.DataFrame()\n",
    "sub_df['qid'] = [qid.vocab.itos[i] for i in test_id]\n",
    "sub_df['prediction'] = (np.array(test_pred) >= delta).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
