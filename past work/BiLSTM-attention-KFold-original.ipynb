{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "710ed17d0c57bd287be0ee3b2782a53a54510561"
   },
   "source": [
    "## IMPORTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_uuid": "abb7e3c30b8a412a50c6b451c49939e3cf4bc11b"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from torchtext import data\n",
    "import spacy\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "tqdm.pandas(desc='Progress')\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from torchtext.data import Example\n",
    "from sklearn.metrics import f1_score\n",
    "import torchtext\n",
    "import os \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure determinism in the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Loading Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.005838499,0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    "    \n",
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.0053247833,0.49346462\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea10c8e218a1280faa9802bcb7f1117c89ec96f9"
   },
   "source": [
    "## LOAD PROCESSED TRAINING DATA FROM DISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 70 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "173753f0178464d2ba26baf22899884d76d1c83d"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")\n",
    "df = pd.concat([df_train ,df_test],sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "5cb425ffbf1f79c1edc4cad3da15a1c1aa53edca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# insincere questions: 1,225,312(93.81%) and # sincere questions: 80,810(6.19%)\n",
      "# Test samples: 56,370(0.043% of train samples)\n"
     ]
    }
   ],
   "source": [
    "sin = len(df_train[df_train[\"target\"]==0])\n",
    "insin = len(df_train[df_train[\"target\"]==1])\n",
    "persin = (sin/(sin+insin))*100\n",
    "perinsin = (insin/(sin+insin))*100            \n",
    "print(\"# insincere questions: {:,}({:.2f}%) and # sincere questions: {:,}({:.2f}%)\".format(sin,persin,insin,perinsin))\n",
    "# print(\"Sinsere:{}% Insincere: {}%\".format(round(persin,2),round(perinsin,2)))\n",
    "print(\"# Test samples: {:,}({:.3f}% of train samples)\".format(len(df_test),len(df_test)/len(df_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07e9890ec0b490cef57565f7dff953aa56ebd3dc"
   },
   "source": [
    "## Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def check_coverage(vocab, embeddings_index):\n",
    "    known_words = {}\n",
    "    unknown_words = {}\n",
    "    nb_known_words = 0\n",
    "    nb_unknown_words = 0\n",
    "    for word in vocab.keys():\n",
    "        try:\n",
    "            known_words[word] = embeddings_index[word]\n",
    "            nb_known_words += vocab[word]\n",
    "        except:\n",
    "            unknown_words[word] = vocab[word]\n",
    "            nb_unknown_words += vocab[word]\n",
    "            pass\n",
    "\n",
    "    print('Found embeddings for {:.2%} of vocab'.format(len(known_words) / len(vocab)))\n",
    "    print('Found embeddings for  {:.2%} of all text'.format(nb_known_words / (nb_known_words + nb_unknown_words)))\n",
    "    unknown_words = sorted(unknown_words.items(), key=operator.itemgetter(1))[::-1]\n",
    "\n",
    "    return unknown_words\n",
    "\n",
    "def known_contractions(embed):\n",
    "    known = []\n",
    "    for contract in contraction_mapping:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x\n",
    "def unknown_punct(embed, punct):\n",
    "    unknown = ''\n",
    "    for p in punct:\n",
    "        if p not in embed:\n",
    "            unknown += p\n",
    "            unknown += ' '\n",
    "    return unknown\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "# vocab = build_vocab(df['question_text'])\n",
    "# add_lower(embed_glove, vocab,embed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "34292d20c25c526e0a440619bf4c5718bf23cfef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1306122/1306122 [00:03<00:00, 331932.84it/s]\n",
      "Progress: 100%|██████████| 1306122/1306122 [00:09<00:00, 133382.73it/s]\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 317529.25it/s]\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 131969.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# df_train['question_text'] = df_train['question_text'].progress_apply(lambda x: x.lower())\n",
    "df_train['question_text'] = df_train['question_text'].progress_apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "# df_train['question_text'] =df_train['question_text'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n",
    "df_train['question_text'] = df_train['question_text'].progress_apply(lambda x: correct_spelling(x, mispell_dict))\n",
    "\n",
    "# df_test['question_text'] = df_test['question_text'].progress_apply(lambda x: x.lower())\n",
    "df_test['question_text'] = df_test['question_text'].progress_apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "# df_test['question_text'] =df_test['question_text'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n",
    "df_test['question_text'] = df_test['question_text'].progress_apply(lambda x: correct_spelling(x, mispell_dict))\n",
    "\n",
    "df_train.to_csv(\"train2.csv\")\n",
    "df_test.to_csv(\"test2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill up the missing values\n",
    "x_train = df_train[\"question_text\"].fillna(\"_##_\").values\n",
    "x_test = df_test[\"question_text\"].fillna(\"_##_\").values\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(x_train))\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Pad the sentences \n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Get the target values\n",
    "y_train = df_train['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95000, 300)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing entries in the embedding are set using np.random.normal so we have to seed here too\n",
    "seed_everything()\n",
    "\n",
    "glove_embeddings = load_glove(tokenizer.word_index)\n",
    "paragram_embeddings = load_para(tokenizer.word_index)\n",
    "\n",
    "embedding_matrix = np.mean([glove_embeddings, paragram_embeddings], axis=0)\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = list(StratifiedKFold(n_splits=5, shuffle=True, random_state=10).split(x_train, y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "efa1688a1dfdb16bf5a13d1d19969ce3141dc130"
   },
   "source": [
    "## Create a pytorch dataset from the train samples and build a vocabulary using embedding vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dcf257f6769ce235fa6cd2866b7ecb65a984c27"
   },
   "source": [
    "## SPLIT DATA TO TRAINiNG AND VALIDATION SETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simple attention layer taken from https://github.com/mttk/rnn-classifier/blob/master/model.py\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "RNNS = ['LSTM', 'GRU']\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, nlayers=1, dropout=0.,\n",
    "                   bidirectional=True, rnn_type='GRU'):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        assert rnn_type in RNNS, 'Use one of the following: {}'.format(str(RNNS))\n",
    "        rnn_cell = getattr(nn, rnn_type) # fetch constructor from torch.nn, cleaner than if\n",
    "        self.rnn = rnn_cell(embedding_dim, hidden_dim, nlayers, \n",
    "                            dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "    def forward(self, input, hidden=None):\n",
    "        return self.rnn(input, hidden)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "    def forward(self, query, keys, values):\n",
    "        # Query = [BxQ] [512, 256]\n",
    "        # Keys = [TxBxK] [50, 512, 256]\n",
    "        # Values = [TxBxV]\n",
    "        # Outputs = a:[TxB], lin_comb:[BxV]\n",
    "\n",
    "        # Here we assume q_dim == k_dim (dot product attention)\n",
    "        query_dim = query.shape[1]\n",
    "        scale = 1. / math.sqrt(query_dim)\n",
    "\n",
    "        query = query.unsqueeze(1)  # [BxQ] -> [Bx1xQ]\n",
    "        keys = keys.transpose(0,1).transpose(1,2) # [TxBxK] -> [BxKxT]\n",
    "        energy = torch.bmm(query, keys) # [Bx1xQ]x[BxKxT] -> [Bx1xT]\n",
    "        energy = F.softmax(energy.mul_(scale), dim=2) # scale, normalize\n",
    "\n",
    "        values = values.transpose(0,1) # [TxBxV] -> [BxTxV]\n",
    "        linear_combination = torch.bmm(energy, values).squeeze(1) #[Bx1xT]x[BxTxV] -> [BxV]\n",
    "        return energy, linear_combination\n",
    "\n",
    "class Classifier(nn.Module):\n",
    "    def __init__(self, embedding, encoder, attention, hidden_dim, num_classes):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.embedding = embedding\n",
    "        self.encoder = encoder\n",
    "        self.attention = attention\n",
    "        self.decoder = nn.Linear(hidden_dim, num_classes)\n",
    "\n",
    "        size = 0\n",
    "        for p in self.parameters():\n",
    "            size += p.nelement()\n",
    "        print('Total param size: {}'.format(size))\n",
    "\n",
    "\n",
    "    def forward(self, input):\n",
    "        outputs, hidden = self.encoder(self.embedding(input))\n",
    "        if isinstance(hidden, tuple): # LSTM\n",
    "            hidden = hidden[1] # take the cell state\n",
    "\n",
    "        if self.encoder.bidirectional: # need to concat the last 2 hidden layers\n",
    "            hidden = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "        else:\n",
    "            hidden = hidden[-1]\n",
    "\n",
    "        # max across T?\n",
    "        # Other options (work worse on a few tests):\n",
    "        # linear_combination, _ = torch.max(outputs, 0)\n",
    "        # linear_combination = torch.mean(outputs, 0)\n",
    "\n",
    "        energy, linear_combination = self.attention(hidden, outputs, outputs) \n",
    "        logits = self.decoder(linear_combination)\n",
    "        return logits, energy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "0dd6f80fe6ca04ed304f792b86088a7105c39ba0"
   },
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, static=True, hidden_dim=128, lstm_layer=2, dropout=0.2):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.embedding = nn.Embedding(max_features, embed_size)       \n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        if static:\n",
    "            self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n",
    "                            hidden_size=hidden_dim,\n",
    "                            num_layers=lstm_layer, \n",
    "                            dropout = dropout,\n",
    "                            bidirectional=True)\n",
    "        self.hidden2label = nn.Linear(hidden_dim*lstm_layer*2, 1)\n",
    "        self.decoder = nn.Linear(64, 1) \n",
    "        self.fc = nn.Linear(hidden_dim*2,64)\n",
    "        self.act = nn.ReLU() ## ADDED ACTIVATION FUNCTIONS\n",
    "        self.sigmoid = nn.Sigmoid() ## ADDED ACTIVATION FUNCTIONS\n",
    "        self.attention = Attention()\n",
    "\n",
    "        self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=256, kernel_size=(fs,self.embedding.embedding_dim)) for fs in [3,4,5]])\n",
    "\n",
    "\n",
    "    def forward(self, sents):\n",
    "        x = self.embedding(sents)\n",
    "        x = self.dropout(x)\n",
    "        x = torch.transpose(x, dim0=1, dim1=0)\n",
    "#         self.out = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "        ######## Attention Layer #################################################\n",
    "        hidden = (c_n)\n",
    "        hidden = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "\n",
    "        energy, linear_combination = self.attention(hidden, lstm_out, lstm_out)\n",
    "\n",
    "        linear_combination = self.act(self.fc(linear_combination)) ## ADDED ACTIVATION FUNCTIONS\n",
    "\n",
    "        logits = self.decoder(self.dropout(linear_combination))\n",
    "        \n",
    "        ##########################################################################\n",
    "\n",
    "        y = self.hidden2label(self.dropout(torch.cat([c_n[i,:, :] for i in range(c_n.shape[0])], dim=1)))\n",
    "        return logits    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_uuid": "4e0cacbdcf3bf1b44b5064f45eea6faae63e4b32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTM(\n",
      "  (dropout): Dropout(p=0.2)\n",
      "  (embedding): Embedding(95000, 300)\n",
      "  (lstm): LSTM(300, 128, num_layers=2, dropout=0.2, bidirectional=True)\n",
      "  (hidden2label): Linear(in_features=512, out_features=1, bias=True)\n",
      "  (decoder): Linear(in_features=64, out_features=1, bias=True)\n",
      "  (fc): Linear(in_features=256, out_features=64, bias=True)\n",
      "  (act): ReLU()\n",
      "  (sigmoid): Sigmoid()\n",
      "  (attention): Attention()\n",
      "  (convs): ModuleList(\n",
      "    (0): Conv2d(1, 256, kernel_size=(3, 300), stride=(1, 1))\n",
      "    (1): Conv2d(1, 256, kernel_size=(4, 300), stride=(1, 1))\n",
      "    (2): Conv2d(1, 256, kernel_size=(5, 300), stride=(1, 1))\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
    "model = BiLSTM(lstm_layer=2,hidden_dim=128,dropout=DROPOUT).cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4e47597cde552a41cdd8ec2531aa6a861e491ae"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/6 \t loss=63.4793 \t val_loss=54.4104 \t time=103.01s\n",
      "Epoch 2/6 \t loss=54.3286 \t val_loss=51.3267 \t time=103.76s\n",
      "Epoch 3/6 \t loss=51.0151 \t val_loss=50.4801 \t time=103.90s\n",
      "Epoch 4/6 \t loss=48.4252 \t val_loss=49.6024 \t time=102.91s\n",
      "Epoch 5/6 \t loss=46.0402 \t val_loss=49.9151 \t time=103.59s\n",
      "Epoch 6/6 \t loss=43.7896 \t val_loss=51.0488 \t time=103.51s\n",
      "Fold 2\n",
      "Epoch 1/6 \t loss=44.3568 \t val_loss=38.8962 \t time=103.39s\n",
      "Epoch 2/6 \t loss=42.1722 \t val_loss=40.0732 \t time=103.95s\n",
      "Epoch 3/6 \t loss=40.0349 \t val_loss=41.1722 \t time=103.51s\n",
      "Epoch 4/6 \t loss=37.9609 \t val_loss=42.2255 \t time=103.62s\n",
      "Epoch 5/6 \t loss=36.3688 \t val_loss=42.9929 \t time=103.52s\n",
      "Epoch 6/6 \t loss=34.7184 \t val_loss=44.8318 \t time=103.65s\n",
      "Fold 3\n",
      "Epoch 1/6 \t loss=37.3979 \t val_loss=27.4783 \t time=103.68s\n",
      "Epoch 2/6 \t loss=35.2980 \t val_loss=29.1788 \t time=103.42s\n",
      "Epoch 3/6 \t loss=33.8385 \t val_loss=31.0196 \t time=103.83s\n",
      "Epoch 4/6 \t loss=32.3626 \t val_loss=32.1350 \t time=103.39s\n",
      "Epoch 5/6 \t loss=31.2325 \t val_loss=33.8012 \t time=103.47s\n",
      "Epoch 6/6 \t loss=30.1176 \t val_loss=35.5077 \t time=103.27s\n",
      "Fold 4\n",
      "Epoch 1/6 \t loss=32.8529 \t val_loss=20.3860 \t time=103.35s\n",
      "Epoch 2/6 \t loss=31.0260 \t val_loss=22.2754 \t time=103.43s\n",
      "Epoch 3/6 \t loss=29.7870 \t val_loss=24.2195 \t time=103.46s\n",
      "Epoch 4/6 \t loss=28.8850 \t val_loss=25.4694 \t time=104.20s\n",
      "Epoch 5/6 \t loss=27.8557 \t val_loss=27.4693 \t time=103.54s\n",
      "Epoch 6/6 \t loss=27.0029 \t val_loss=28.5403 \t time=103.29s\n",
      "Fold 5\n",
      "Epoch 1/6 \t loss=29.9157 \t val_loss=16.1083 \t time=103.42s\n",
      "Epoch 2/6 \t loss=28.4338 \t val_loss=18.6352 \t time=103.49s\n",
      "Epoch 3/6 \t loss=27.4513 \t val_loss=19.3162 \t time=103.20s\n",
      "Epoch 4/6 \t loss=26.5705 \t val_loss=20.4335 \t time=103.43s\n",
      "Epoch 5/6 \t loss=25.6783 \t val_loss=22.6591 \t time=103.39s\n",
      "Epoch 6/6 \t loss=25.1931 \t val_loss=23.4802 \t time=103.37s\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "batch_size = 512 # how many samples to process at once\n",
    "n_epochs = 6 # how many times to iterate over all samples\n",
    "\n",
    "# matrix for the out-of-fold predictions\n",
    "train_preds = np.zeros((len(df_train)))\n",
    "# matrix for the predictions on the test set\n",
    "test_preds = np.zeros((len(df_test)))\n",
    "\n",
    "# always call this before training for deterministic results\n",
    "seed_everything()\n",
    "\n",
    "x_test_cuda = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):    \n",
    "    # split data in train / validation according to the KFold indeces\n",
    "    # also, convert them to a torch tensor and store them on the GPU (done with .cuda())\n",
    "    x_train_fold = torch.tensor(x_train[train_idx], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(y_train[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    x_val_fold = torch.tensor(x_train[valid_idx], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(y_train[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "#     model = NeuralNet()\n",
    "    # make sure everything in the model is running on the GPU\n",
    "    model.cuda()\n",
    "\n",
    "    # define binary cross entropy loss\n",
    "    # note that the model returns logit to take advantage of the log-sum-exp trick \n",
    "    # for numerical stability in the loss\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # set train mode of the model. This enables operations which are only applied during training like dropout\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.  \n",
    "        for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "            # Forward pass: compute predicted y by passing x to the model.\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            # Compute and print loss.\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the Tensors it will update (which are the learnable weights\n",
    "            # of the model)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its parameters\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        # set evaluation mode of the model. This disabled operations which are only applied during training like dropout\n",
    "        model.eval()\n",
    "        \n",
    "        # predict all the samples in y_val_fold batch per batch\n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros((len(df_test)))\n",
    "        \n",
    "        avg_val_loss = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            y_pred = model(x_batch).detach()\n",
    "            \n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        \n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "        \n",
    "    # predict all samples in the test set batch per batch\n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds += test_preds_fold / len(splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_uuid": "dc4a4c681294ba06526fed4e871cfe8639cf25e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:06<00:00,  6.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold is 0.3900 with F1 score: 0.7705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = [0,0,0] # idx, cur, max\n",
    "delta = 0\n",
    "for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n",
    "    tmp[1] = f1_score(y_train, np.array(train_preds)>tmp[0])\n",
    "    if tmp[1] > tmp[2]:\n",
    "        delta = tmp[0]\n",
    "        tmp[2] = tmp[1]\n",
    "print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_uuid": "d17dd7b0a92ec98134bf8996fd210edaadf7bed6"
   },
   "outputs": [],
   "source": [
    "submission = df_test[['qid']].copy()\n",
    "submission['prediction'] = (test_preds > delta).astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "_uuid": "a1a07a65e47b5dcefaac040f3f633dc077e8f61e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "qid,prediction\r\n",
      "00014894849d00ba98a9,0\r\n",
      "000156468431f09b3cae,0\r\n",
      "000227734433360e1aae,0\r\n",
      "0005e06fbe3045bd2a92,0\r\n",
      "00068a0f7f41f50fc399,0\r\n",
      "000a2d30e3ffd70c070d,0\r\n",
      "000b67672ec9622ff761,0\r\n",
      "000b7fb1146d712c1105,0\r\n",
      "000d665a8ddc426a1907,0\r\n"
     ]
    }
   ],
   "source": [
    "!head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
