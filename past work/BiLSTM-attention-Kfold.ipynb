{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preface\n",
    "\n",
    "* https://www.kaggle.com/gmhost/gru-capsule\n",
    "* How to: Preprocessing when using embeddings\n",
    "https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings\n",
    "* Improve your Score with some Text Preprocessing https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing\n",
    "* Simple attention layer taken from https://github.com/mttk/rnn-classifier/blob/master/model.py\n",
    "* https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm\n",
    "* https://www.kaggle.com/hengzheng/pytorch-starter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "710ed17d0c57bd287be0ee3b2782a53a54510561"
   },
   "source": [
    "## IMPORTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "_uuid": "abb7e3c30b8a412a50c6b451c49939e3cf4bc11b"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import torch\n",
    "from torchtext import data\n",
    "import spacy\n",
    "from tqdm import tqdm, tqdm_notebook, tnrange\n",
    "tqdm.pandas(desc='Progress')\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n",
    "from nltk import word_tokenize\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from torch.autograd import Variable\n",
    "from torchtext.data import Example\n",
    "from sklearn.metrics import f1_score\n",
    "import torchtext\n",
    "import os \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# cross validation and metrics\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensure determinism in the results\n",
    "\n",
    "A common headache in this competition is the lack of determinism in the results due to cudnn. The following Kernel has a solution in Pytorch.\n",
    "\n",
    "See https://www.kaggle.com/hengzheng/pytorch-starter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "seed_everything()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for Loading Embeddings\n",
    "\n",
    "Functions taken from the kernel:https://www.kaggle.com/gmhost/gru-capsule\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "## FUNCTIONS TAKEN FROM https://www.kaggle.com/gmhost/gru-capsule\n",
    "\n",
    "def load_glove(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')[:300]\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n",
    "    \n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.005838499,0.48782197\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "            \n",
    "    return embedding_matrix \n",
    "    \n",
    "def load_fasttext(word_index):    \n",
    "    EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "\n",
    "    return embedding_matrix\n",
    "\n",
    "def load_para(word_index):\n",
    "    EMBEDDING_FILE = '../input/embeddings/paragram_300_sl999/paragram_300_sl999.txt'\n",
    "    def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "    embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE, encoding=\"utf8\", errors='ignore') if len(o)>100)\n",
    "\n",
    "    all_embs = np.stack(embeddings_index.values())\n",
    "    emb_mean,emb_std = -0.0053247833,0.49346462\n",
    "    embed_size = all_embs.shape[1]\n",
    "\n",
    "    # word_index = tokenizer.word_index\n",
    "    nb_words = min(max_features, len(word_index))\n",
    "    embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "    for word, i in word_index.items():\n",
    "        if i >= max_features: continue\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n",
    "    \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ea10c8e218a1280faa9802bcb7f1117c89ec96f9"
   },
   "source": [
    "## LOAD PROCESSED TRAINING DATA FROM DISK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 95000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 70 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "_uuid": "173753f0178464d2ba26baf22899884d76d1c83d"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(\"../input/train.csv\")\n",
    "df_test = pd.read_csv(\"../input/test.csv\")\n",
    "df = pd.concat([df_train ,df_test],sort=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "_uuid": "5cb425ffbf1f79c1edc4cad3da15a1c1aa53edca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# insincere questions: 1,225,312(93.81%) and # sincere questions: 80,810(6.19%)\n",
      "# Test samples: 56,370(0.043% of train samples)\n"
     ]
    }
   ],
   "source": [
    "sin = len(df_train[df_train[\"target\"]==0])\n",
    "insin = len(df_train[df_train[\"target\"]==1])\n",
    "persin = (sin/(sin+insin))*100\n",
    "perinsin = (insin/(sin+insin))*100            \n",
    "print(\"# insincere questions: {:,}({:.2f}%) and # sincere questions: {:,}({:.2f}%)\".format(sin,persin,insin,perinsin))\n",
    "# print(\"Sinsere:{}% Insincere: {}%\".format(round(persin,2),round(perinsin,2)))\n",
    "print(\"# Test samples: {:,}({:.3f}% of train samples)\".format(len(df_test),len(df_test)/len(df_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "07e9890ec0b490cef57565f7dff953aa56ebd3dc"
   },
   "source": [
    "## Normalization\n",
    "\n",
    "Borrowed from:\n",
    "* How to: Preprocessing when using embeddings\n",
    "https://www.kaggle.com/christofhenkel/how-to-preprocessing-when-using-embeddings\n",
    "* Improve your Score with some Text Preprocessing https://www.kaggle.com/theoviel/improve-your-score-with-some-text-preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_vocab(texts):\n",
    "    sentences = texts.apply(lambda x: x.split()).values\n",
    "    vocab = {}\n",
    "    for sentence in sentences:\n",
    "        for word in sentence:\n",
    "            try:\n",
    "                vocab[word] += 1\n",
    "            except KeyError:\n",
    "                vocab[word] = 1\n",
    "    return vocab\n",
    "\n",
    "def known_contractions(embed):\n",
    "    known = []\n",
    "    for contract in contraction_mapping:\n",
    "        if contract in embed:\n",
    "            known.append(contract)\n",
    "    return known\n",
    "def clean_contractions(text, mapping):\n",
    "    specials = [\"’\", \"‘\", \"´\", \"`\"]\n",
    "    for s in specials:\n",
    "        text = text.replace(s, \"'\")\n",
    "    text = ' '.join([mapping[t] if t in mapping else t for t in text.split(\" \")])\n",
    "    return text\n",
    "def correct_spelling(x, dic):\n",
    "    for word in dic.keys():\n",
    "        x = x.replace(word, dic[word])\n",
    "    return x\n",
    "def unknown_punct(embed, punct):\n",
    "    unknown = ''\n",
    "    for p in punct:\n",
    "        if p not in embed:\n",
    "            unknown += p\n",
    "            unknown += ' '\n",
    "    return unknown\n",
    "def clean_special_chars(text, punct, mapping):\n",
    "    for p in mapping:\n",
    "        text = text.replace(p, mapping[p])\n",
    "    \n",
    "    for p in punct:\n",
    "        text = text.replace(p, f' {p} ')\n",
    "    \n",
    "    specials = {'\\u200b': ' ', '…': ' ... ', '\\ufeff': '', 'करना': '', 'है': ''}  # Other special characters that I have to deal with in last\n",
    "    for s in specials:\n",
    "        text = text.replace(s, specials[s])\n",
    "    \n",
    "    return text\n",
    "def add_lower(embedding, vocab):\n",
    "    count = 0\n",
    "    for word in vocab:\n",
    "        if word in embedding and word.lower() not in embedding:  \n",
    "            embedding[word.lower()] = embedding[word]\n",
    "            count += 1\n",
    "    print(f\"Added {count} words to embedding\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\", \"didn't\": \"did not\",  \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",  \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\", \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",  \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" }\n",
    "punct = \"/-'?!.,#$%\\'()*+-/:;<=>@[\\\\]^_`{|}~\" + '\"\"“”’' + '∞θ÷α•à−β∅³π‘₹´°£€\\×™√²—–&'\n",
    "punct_mapping = {\"‘\": \"'\", \"₹\": \"e\", \"´\": \"'\", \"°\": \"\", \"€\": \"e\", \"™\": \"tm\", \"√\": \" sqrt \", \"×\": \"x\", \"²\": \"2\", \"—\": \"-\", \"–\": \"-\", \"’\": \"'\", \"_\": \"-\", \"`\": \"'\", '“': '\"', '”': '\"', '“': '\"', \"£\": \"e\", '∞': 'infinity', 'θ': 'theta', '÷': '/', 'α': 'alpha', '•': '.', 'à': 'a', '−': '-', 'β': 'beta', '∅': '', '³': '3', 'π': 'pi', }\n",
    "mispell_dict = {'colour': 'color', 'centre': 'center', 'favourite': 'favorite', 'travelling': 'traveling', 'counselling': 'counseling', 'theatre': 'theater', 'cancelled': 'canceled', 'labour': 'labor', 'organisation': 'organization', 'wwii': 'world war 2', 'citicise': 'criticize', 'youtu ': 'youtube ', 'Qoura': 'Quora', 'sallary': 'salary', 'Whta': 'What', 'narcisist': 'narcissist', 'howdo': 'how do', 'whatare': 'what are', 'howcan': 'how can', 'howmuch': 'how much', 'howmany': 'how many', 'whydo': 'why do', 'doI': 'do I', 'theBest': 'the best', 'howdoes': 'how does', 'mastrubation': 'masturbation', 'mastrubate': 'masturbate', \"mastrubating\": 'masturbating', 'pennis': 'penis', 'Etherium': 'Ethereum', 'narcissit': 'narcissist', 'bigdata': 'big data', '2k17': '2017', '2k18': '2018', 'qouta': 'quota', 'exboyfriend': 'ex boyfriend', 'airhostess': 'air hostess', \"whst\": 'what', 'watsapp': 'whatsapp', 'demonitisation': 'demonetization', 'demonitization': 'demonetization', 'demonetisation': 'demonetization'}\n",
    "\n",
    "# vocab = build_vocab(df['question_text'])\n",
    "# add_lower(embed_glove, vocab,embed_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "_uuid": "34292d20c25c526e0a440619bf4c5718bf23cfef"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progress: 100%|██████████| 1306122/1306122 [00:04<00:00, 322911.73it/s]\n",
      "Progress: 100%|██████████| 1306122/1306122 [00:10<00:00, 130184.08it/s]\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 303787.07it/s]\n",
      "Progress: 100%|██████████| 56370/56370 [00:00<00:00, 128454.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# df_train['question_text'] = df_train['question_text'].progress_apply(lambda x: x.lower())\n",
    "df_train['question_text'] = df_train['question_text'].progress_apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "# df_train['question_text'] =df_train['question_text'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n",
    "df_train['question_text'] = df_train['question_text'].progress_apply(lambda x: correct_spelling(x, mispell_dict))\n",
    "\n",
    "# df_test['question_text'] = df_test['question_text'].progress_apply(lambda x: x.lower())\n",
    "df_test['question_text'] = df_test['question_text'].progress_apply(lambda x: clean_contractions(x, contraction_mapping))\n",
    "# df_test['question_text'] =df_test['question_text'].progress_apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n",
    "df_test['question_text'] = df_test['question_text'].progress_apply(lambda x: correct_spelling(x, mispell_dict))\n",
    "\n",
    "df_train.to_csv(\"train2.csv\")\n",
    "df_test.to_csv(\"test2.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill up the missing values\n",
    "x_train = df_train[\"question_text\"].fillna(\"_##_\").values\n",
    "x_test = df_test[\"question_text\"].fillna(\"_##_\").values\n",
    "\n",
    "# Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(x_train))\n",
    "x_train = tokenizer.texts_to_sequences(x_train)\n",
    "x_test = tokenizer.texts_to_sequences(x_test)\n",
    "\n",
    "# Pad the sentences \n",
    "x_train = pad_sequences(x_train, maxlen=maxlen)\n",
    "x_test = pad_sequences(x_test, maxlen=maxlen)\n",
    "\n",
    "# Get the target values\n",
    "y_train = df_train['target'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Embeddings\n",
    "\n",
    "Two embedding matrices have been used. Glove, and paragram. The mean of the two is used as the final embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95000, 300)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing entries in the embedding are set using np.random.normal so we have to seed here too\n",
    "seed_everything()\n",
    "\n",
    "glove_embeddings = load_glove(tokenizer.word_index)\n",
    "paragram_embeddings = load_para(tokenizer.word_index)\n",
    "\n",
    "embedding_matrix = np.mean([glove_embeddings, paragram_embeddings], axis=0)\n",
    "np.shape(embedding_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Stratified K Fold to improve results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([      1,       2,       4, ..., 1306118, 1306120, 1306121]),\n",
       "  array([      0,       3,       5, ..., 1306115, 1306117, 1306119])),\n",
       " (array([      0,       1,       2, ..., 1306119, 1306120, 1306121]),\n",
       "  array([      4,       8,      10, ..., 1306109, 1306112, 1306114])),\n",
       " (array([      0,       2,       3, ..., 1306117, 1306119, 1306120]),\n",
       "  array([      1,       9,      26, ..., 1306111, 1306118, 1306121])),\n",
       " (array([      0,       1,       3, ..., 1306118, 1306119, 1306121]),\n",
       "  array([      2,      12,      13, ..., 1306108, 1306116, 1306120]))]"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "splits = list(StratifiedKFold(n_splits=4, shuffle=True, random_state=10).split(x_train, y_train))\n",
    "splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, feature_dim, step_dim, bias=True, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        \n",
    "        self.supports_masking = True\n",
    "\n",
    "        self.bias = bias\n",
    "        self.feature_dim = feature_dim\n",
    "        self.step_dim = step_dim\n",
    "        self.features_dim = 0\n",
    "        \n",
    "        weight = torch.zeros(feature_dim, 1)\n",
    "        nn.init.xavier_uniform_(weight)\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        \n",
    "        if bias:\n",
    "            self.b = nn.Parameter(torch.zeros(step_dim))\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        feature_dim = self.feature_dim\n",
    "        step_dim = self.step_dim\n",
    "\n",
    "        eij = torch.mm(\n",
    "            x.contiguous().view(-1, feature_dim), \n",
    "            self.weight\n",
    "        ).view(-1, step_dim)\n",
    "        \n",
    "        if self.bias:\n",
    "            eij = eij + self.b\n",
    "            \n",
    "        eij = torch.tanh(eij)\n",
    "        a = torch.exp(eij)\n",
    "        \n",
    "        if mask is not None:\n",
    "            a = a * mask\n",
    "\n",
    "        a = a / torch.sum(a, 1, keepdim=True) + 1e-10\n",
    "\n",
    "        weighted_input = x * torch.unsqueeze(a, -1)\n",
    "        return torch.sum(weighted_input, 1)\n",
    "    \n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        \n",
    "        hidden_size = 40\n",
    "        \n",
    "        self.embedding = nn.Embedding(max_features, embed_size)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        \n",
    "        self.embedding_dropout = nn.Dropout2d(0.1)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, bidirectional=True, batch_first=True)\n",
    "        self.gru = nn.GRU(hidden_size * 2, hidden_size, bidirectional=True, batch_first=True)\n",
    "        \n",
    "        self.lstm_attention = Attention(hidden_size * 2, maxlen)\n",
    "        self.gru_attention = Attention(hidden_size * 2, maxlen)\n",
    "        \n",
    "        self.linear = nn.Linear(320, 64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.out = nn.Linear(64, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        h_embedding = self.embedding(x)\n",
    "        h_embedding = torch.squeeze(\n",
    "            self.embedding_dropout(torch.unsqueeze(h_embedding, 0)))\n",
    "        \n",
    "        h_lstm, _ = self.lstm(h_embedding)\n",
    "        h_gru, _ = self.gru(h_lstm)\n",
    "        \n",
    "        h_lstm_atten = self.lstm_attention(h_lstm)\n",
    "        h_gru_atten = self.gru_attention(h_gru)\n",
    "        \n",
    "        # global average pooling\n",
    "        avg_pool = torch.mean(h_gru, 1)\n",
    "        # global max pooling\n",
    "        max_pool, _ = torch.max(h_gru, 1)\n",
    "        \n",
    "        conc = torch.cat((h_lstm_atten, h_gru_atten, avg_pool, max_pool), 1)\n",
    "        conc = self.relu(self.linear(conc))\n",
    "        conc = self.dropout(conc)\n",
    "        out = self.out(conc)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Simple attention layer taken from https://github.com/mttk/rnn-classifier/blob/master/model.py\n",
    "\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import math\n",
    "\n",
    "# RNNS = ['LSTM', 'GRU']\n",
    "\n",
    "# class Encoder(nn.Module):\n",
    "#     def __init__(self, embedding_dim, hidden_dim, nlayers=1, dropout=0.,\n",
    "#                    bidirectional=True, rnn_type='GRU'):\n",
    "#         super(Encoder, self).__init__()\n",
    "#         self.bidirectional = bidirectional\n",
    "#         assert rnn_type in RNNS, 'Use one of the following: {}'.format(str(RNNS))\n",
    "#         rnn_cell = getattr(nn, rnn_type) # fetch constructor from torch.nn, cleaner than if\n",
    "#         self.rnn = rnn_cell(embedding_dim, hidden_dim, nlayers, \n",
    "#                             dropout=dropout, bidirectional=bidirectional)\n",
    "\n",
    "#     def forward(self, input, hidden=None):\n",
    "#         return self.rnn(input, hidden)\n",
    "\n",
    "\n",
    "# class Attention(nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(Attention, self).__init__()\n",
    "\n",
    "#     def forward(self, query, keys, values):\n",
    "#         # Query = [BxQ] [512, 256]\n",
    "#         # Keys = [TxBxK] [50, 512, 256]\n",
    "#         # Values = [TxBxV]\n",
    "#         # Outputs = a:[TxB], lin_comb:[BxV]\n",
    "\n",
    "#         # Here we assume q_dim == k_dim (dot product attention)\n",
    "#         query_dim = query.shape[1]\n",
    "#         scale = 1. / math.sqrt(query_dim)\n",
    "\n",
    "#         query = query.unsqueeze(1)  # [BxQ] -> [Bx1xQ]\n",
    "#         keys = keys.transpose(0,1).transpose(1,2) # [TxBxK] -> [BxKxT]\n",
    "#         energy = torch.bmm(query, keys) # [Bx1xQ]x[BxKxT] -> [Bx1xT]\n",
    "#         energy = F.softmax(energy.mul_(scale), dim=2) # scale, normalize\n",
    "\n",
    "#         values = values.transpose(0,1) # [TxBxV] -> [BxTxV]\n",
    "#         linear_combination = torch.bmm(energy, values).squeeze(1) #[Bx1xT]x[BxTxV] -> [BxV]\n",
    "#         return energy, linear_combination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n",
    "\n",
    "Binary LSTM with an attention layer and an additional fully connected layer.\n",
    "\n",
    "Initial idea borrowed from: https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "_uuid": "0dd6f80fe6ca04ed304f792b86088a7105c39ba0"
   },
   "outputs": [],
   "source": [
    "# class BiLSTM(nn.Module):\n",
    "#     def __init__(self, static=True, hidden_dim=128, lstm_layer=2, dropout=0.2):\n",
    "#         super(BiLSTM, self).__init__()\n",
    "#         self.hidden_dim = hidden_dim\n",
    "#         self.dropout = nn.Dropout(p=dropout)\n",
    "#         self.embedding = nn.Embedding(max_features, embed_size)       \n",
    "#         self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "#         if static:\n",
    "#             self.embedding.weight.requires_grad = False\n",
    "#         self.lstm = nn.LSTM(input_size=self.embedding.embedding_dim,\n",
    "#                             hidden_size=hidden_dim,\n",
    "#                             num_layers=lstm_layer, \n",
    "#                             dropout = dropout,\n",
    "#                             bidirectional=True)\n",
    "#         self.hidden2label = nn.Linear(hidden_dim*lstm_layer*2, 1)\n",
    "#         self.decoder = nn.Linear(64, 1) \n",
    "#         self.fc = nn.Linear(hidden_dim*2,64)\n",
    "#         self.act = nn.ReLU() ## ADDED ACTIVATION FUNCTIONS\n",
    "#         self.sigmoid = nn.Sigmoid() ## ADDED ACTIVATION FUNCTIONS\n",
    "#         self.attention = Attention()\n",
    "#         self.embedding_dropout = nn.Dropout2d(0.2)\n",
    "\n",
    "#         self.convs = nn.ModuleList([nn.Conv2d(in_channels=1, out_channels=256, kernel_size=(fs,self.embedding.embedding_dim)) for fs in [3,4,5]])\n",
    "\n",
    "#     def forward(self, sents):\n",
    "#         x = self.embedding(sents)\n",
    "#         x = torch.squeeze(\n",
    "#         self.embedding_dropout(torch.unsqueeze(x, 0)))\n",
    "\n",
    "# #         self.out = nn.Linear(hidden_dim, num_classes)\n",
    "        \n",
    "#         lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "\n",
    "#         ######## Attention Layer #################################################\n",
    "#         hidden = (c_n)\n",
    "#         hidden = torch.cat([hidden[-1], hidden[-2]], dim=1)\n",
    "\n",
    "#         energy, linear_combination = self.attention(hidden, lstm_out, lstm_out)\n",
    "\n",
    "#         linear_combination = self.act(self.fc(linear_combination)) ## ADDED ACTIVATION FUNCTIONS\n",
    "\n",
    "#         logits = self.decoder(self.dropout(linear_combination))\n",
    "        \n",
    "#         ##########################################################################\n",
    "\n",
    "#         y = self.hidden2label(self.dropout(torch.cat([c_n[i,:, :] for i in range(c_n.shape[0])], dim=1)))\n",
    "#         return logits    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "_uuid": "4e0cacbdcf3bf1b44b5064f45eea6faae63e4b32"
   },
   "outputs": [],
   "source": [
    "# INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 300\n",
    "N_FILTERS = 100\n",
    "OUTPUT_DIM = 1\n",
    "DROPOUT = 0.2\n",
    "\n",
    "# model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
    "# model = BiLSTM(lstm_layer=2,hidden_dim=128,dropout=DROPOUT).cuda()\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "e4e47597cde552a41cdd8ec2531aa6a861e491ae"
   },
   "source": [
    "### Training\n",
    "\n",
    "The method is borrowed from https://www.kaggle.com/hengzheng/pytorch-starter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1\n",
      "Epoch 1/5 \t loss=67.8133 \t val_loss=54.0512 \t time=38.01s\n",
      "Epoch 2/5 \t loss=59.3448 \t val_loss=52.0314 \t time=38.10s\n",
      "Epoch 3/5 \t loss=56.4401 \t val_loss=50.9484 \t time=37.94s\n",
      "Epoch 4/5 \t loss=54.0541 \t val_loss=51.0588 \t time=37.95s\n",
      "Epoch 5/5 \t loss=52.1897 \t val_loss=50.7644 \t time=38.08s\n",
      "Fold 2\n",
      "Epoch 1/5 \t loss=67.6046 \t val_loss=54.4906 \t time=38.16s\n",
      "Epoch 2/5 \t loss=59.2771 \t val_loss=51.8503 \t time=38.09s\n",
      "Epoch 3/5 \t loss=56.7447 \t val_loss=51.3499 \t time=38.16s\n",
      "Epoch 4/5 \t loss=54.6345 \t val_loss=50.9205 \t time=38.11s\n",
      "Epoch 5/5 \t loss=52.7191 \t val_loss=50.9253 \t time=38.11s\n",
      "Fold 3\n",
      "Epoch 1/5 \t loss=66.9698 \t val_loss=55.1586 \t time=38.04s\n",
      "Epoch 2/5 \t loss=59.1721 \t val_loss=52.5167 \t time=37.87s\n",
      "Epoch 3/5 \t loss=56.2718 \t val_loss=51.6099 \t time=38.04s\n",
      "Epoch 4/5 \t loss=54.2680 \t val_loss=51.3284 \t time=37.95s\n",
      "Epoch 5/5 \t loss=52.1218 \t val_loss=51.6435 \t time=37.95s\n",
      "Fold 4\n",
      "Epoch 1/5 \t loss=67.2178 \t val_loss=54.6352 \t time=38.10s\n",
      "Epoch 2/5 \t loss=58.9369 \t val_loss=52.9309 \t time=37.90s\n",
      "Epoch 3/5 \t loss=56.3332 \t val_loss=51.6237 \t time=38.11s\n",
      "Epoch 4/5 \t loss=54.2395 \t val_loss=51.7540 \t time=38.12s\n",
      "Epoch 5/5 \t loss=52.2685 \t val_loss=52.8173 \t time=38.09s\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "batch_size = 512 # how many samples to process at once\n",
    "n_epochs = 5 # how many times to iterate over all samples\n",
    "\n",
    "# matrix for the out-of-fold predictions\n",
    "train_preds = np.zeros((len(df_train)))\n",
    "# matrix for the predictions on the test set\n",
    "test_preds = np.zeros((len(df_test)))\n",
    "\n",
    "# always call this before training for deterministic results\n",
    "seed_everything()\n",
    "\n",
    "x_test_cuda = torch.tensor(x_test, dtype=torch.long).cuda()\n",
    "test = torch.utils.data.TensorDataset(x_test_cuda)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(splits):    \n",
    "    # split data in train / validation according to the KFold indeces\n",
    "    # also, convert them to a torch tensor and store them on the GPU (done with .cuda())\n",
    "    x_train_fold = torch.tensor(x_train[train_idx], dtype=torch.long).cuda()\n",
    "    y_train_fold = torch.tensor(y_train[train_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    x_val_fold = torch.tensor(x_train[valid_idx], dtype=torch.long).cuda()\n",
    "    y_val_fold = torch.tensor(y_train[valid_idx, np.newaxis], dtype=torch.float32).cuda()\n",
    "    \n",
    "#     model = BiLSTM(lstm_layer=2,hidden_dim=40,dropout=DROPOUT).cuda()\n",
    "    model = NeuralNet()\n",
    "\n",
    "    # make sure everything in the model is running on the GPU\n",
    "    model.cuda()\n",
    "\n",
    "    # define binary cross entropy loss\n",
    "    # note that the model returns logit to take advantage of the log-sum-exp trick \n",
    "    # for numerical stability in the loss\n",
    "    loss_fn = torch.nn.BCEWithLogitsLoss(reduction='sum')\n",
    "    optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    train = torch.utils.data.TensorDataset(x_train_fold, y_train_fold)\n",
    "    valid = torch.utils.data.TensorDataset(x_val_fold, y_val_fold)\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = torch.utils.data.DataLoader(valid, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    print(f'Fold {i + 1}')\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        # set train mode of the model. This enables operations which are only applied during training like dropout\n",
    "        start_time = time.time()\n",
    "        model.train()\n",
    "        avg_loss = 0.  \n",
    "        for x_batch, y_batch in tqdm(train_loader, disable=True):\n",
    "            # Forward pass: compute predicted y by passing x to the model.\n",
    "            y_pred = model(x_batch)\n",
    "\n",
    "            # Compute and print loss.\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "            # Before the backward pass, use the optimizer object to zero all of the\n",
    "            # gradients for the Tensors it will update (which are the learnable weights\n",
    "            # of the model)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Backward pass: compute gradient of the loss with respect to model parameters\n",
    "            loss.backward()\n",
    "\n",
    "            # Calling the step function on an Optimizer makes an update to its parameters\n",
    "            optimizer.step()\n",
    "            avg_loss += loss.item() / len(train_loader)\n",
    "            \n",
    "        # set evaluation mode of the model. This disabled operations which are only applied during training like dropout\n",
    "        model.eval()\n",
    "        \n",
    "        # predict all the samples in y_val_fold batch per batch\n",
    "        valid_preds_fold = np.zeros((x_val_fold.size(0)))\n",
    "        test_preds_fold = np.zeros((len(df_test)))\n",
    "        \n",
    "        avg_val_loss = 0.\n",
    "        for i, (x_batch, y_batch) in enumerate(valid_loader):\n",
    "            y_pred = model(x_batch).detach()\n",
    "            \n",
    "            avg_val_loss += loss_fn(y_pred, y_batch).item() / len(valid_loader)\n",
    "            valid_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "        \n",
    "        elapsed_time = time.time() - start_time \n",
    "        print('Epoch {}/{} \\t loss={:.4f} \\t val_loss={:.4f} \\t time={:.2f}s'.format(\n",
    "            epoch + 1, n_epochs, avg_loss, avg_val_loss, elapsed_time))\n",
    "        \n",
    "    # predict all samples in the test set batch per batch\n",
    "    for i, (x_batch,) in enumerate(test_loader):\n",
    "        y_pred = model(x_batch).detach()\n",
    "\n",
    "        test_preds_fold[i * batch_size:(i+1) * batch_size] = sigmoid(y_pred.cpu().numpy())[:, 0]\n",
    "\n",
    "    train_preds[valid_idx] = valid_preds_fold\n",
    "    test_preds += test_preds_fold / len(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find final Thresshold\n",
    "\n",
    "Borrowed from: https://www.kaggle.com/ziliwang/baseline-pytorch-bilstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "_uuid": "dc4a4c681294ba06526fed4e871cfe8639cf25e4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [00:06<00:00,  6.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best threshold is 0.3400 with F1 score: 0.6754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tmp = [0,0,0] # idx, cur, max\n",
    "delta = 0\n",
    "for tmp[0] in tqdm(np.arange(0.1, 0.501, 0.01)):\n",
    "    tmp[1] = f1_score(y_train, np.array(train_preds)>tmp[0])\n",
    "    if tmp[1] > tmp[2]:\n",
    "        delta = tmp[0]\n",
    "        tmp[2] = tmp[1]\n",
    "print('best threshold is {:.4f} with F1 score: {:.4f}'.format(delta, tmp[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "_uuid": "d17dd7b0a92ec98134bf8996fd210edaadf7bed6"
   },
   "outputs": [],
   "source": [
    "submission = df_test[['qid']].copy()\n",
    "submission['prediction'] = (test_preds > delta).astype(int)\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "a1a07a65e47b5dcefaac040f3f633dc077e8f61e"
   },
   "outputs": [],
   "source": [
    "# !head submission.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
